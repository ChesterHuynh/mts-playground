{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the tutorials on the <a href=\"https://hmmlearn.readthedocs.io/en/stable/#user-guide-table-of-contents\">hmmlearn docs</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "The HMM is a generative probabilistic model. Under this framework, we have a sequence of _observable_ $X$ variables generated by a sequence of _internal hidden states_ $Z$. As the name implies, hidden states are not observed directly. \n",
    "\n",
    "_Transitions_ between hidden states are assumed to have the form of a (first-order) Markov chain. The transitions can be specified by a start probability vector $\\pi$ and an unchanging transition probability matrix $A$.\n",
    "\n",
    "The _emission probability_ of an observable $x_t$ can be any distribution with parameters $\\theta$ conditioned on the current hidden state $z_t$.\n",
    "\n",
    "An HMM is fully specified by $\\pi$, $A$, and $\\theta$.\n",
    "\n",
    "There are three fundamental problems for HMMs:\n",
    "1. Given the model parameters and observed data, estimate the optimal sequence of hidden states.\n",
    "2. Given the model parameters and observed data, calculate the model likelihood.\n",
    "3. Given just the observed data, estimate the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wgsegment",
   "language": "python",
   "name": "wgsegment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
